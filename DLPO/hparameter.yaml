train:
  batch_size: 1
  adam:
    # lr: 3e-4
    lr: 3e-5
    weight_decay: 1e-6
  decay:
    rate: 0.05
    start: 25000
    end: 100000
  num_workers: 2 #num_workers == gpus
  # gpus: 0 #ddp
  # gpus: 4 #ddp
  nodes: 2
  #sh10_sbatch_pitzer.sh SBATCH --ntasks-per-node=2 match with gpus: 2
  #SBATCH --nodes=1 match with nodes:1
  #SBATCH --gres=gpu:v100-32g:2 match with gpus: 2


  loss_rate:
    dur: 1.0
    mos: 1.0
    asr: 1.0
  # training_mode: 'pretrain', 'finetune','finetune_simple'
  training_mode: 'pretrain'
  x_amount: 10
  v0: "average"

data:
  lang: 'eng'
  text_cleaners: ['english_cleaners'] # korean_cleaners, english_cleaners, chinese_cleaners
  speakers: ['LJSpeech']
  # train_dir: '/users/PAS2062/delijingyic/project/wavegrad2/dataset/LJSpeech/LJSpeech-1.1' 
  train_dir: '/users/PAS2062/delijingyic/project/wavegrad2/dataset/LJSpeech/preprocessed'
  train_meta: 'train_all.txt'  # relative path of metadata file from train_dir
  val_dir: '/users/PAS2062/delijingyic/project/wavegrad2/dataset/LJSpeech/preprocessed'
  val_meta: 'val.txt'  # relative path of metadata file from val_dir'
  lexicon_path: 'lexicon/librispeech-lexicon.txt'
  mosscore_dir: '/users/PAS2062/delijingyic/project/wavegrad2/mos_results/10_07_12_epoch=27MOS120_WG21059_base.json'
  # mosscore_dir: '/users/PAS2062/delijingyic/project/wavegrad2/mos_results/10_07_12_epoch=27MOS120_WG21059_base_standarized.json'
  
audio:
  n_mel_channels: 80
  filter_length: 1024
  hop_length: 256
  win_length: 1024
  sampling_rate: 22050
  mel_fmin: 0.0
  mel_fmax: 8000.0
encoder:
  channel: 512
  kernel: 5
  depth: 3
  dropout_rate: 0.5
  speaker_emb: 64

dur_predictor:
  dur_lstm_channel: 512
  range_lstm_channel: 512  

window:
  scale: 300
  length: 256

wavegrad:
  # is_large: True #if False, Base
  is_large: False #if False, Base
  encode_channel: 576 ##512+64
  scale_factors: [5,5,3,2,2]
  upsample:
    preconv_channel: 768
    out_channels: [512, 512, 256, 128, 128] 
    # dilations: [[1,2,4,8],[1,2,4,8],[1,2,4,8],[1,2,4,8],[1,2,4,8]] 
    # dilations for large
    dilations: [[1,2,4,8],[1,2,4,8],[1,2,4,8],[1,2,1,2],[1,2,1,2]]
    #dilations for small

  downsample:
    preconv_channel: 32
    out_channels: [128, 128, 256, 512]
    dilations: [[1,2,4],[1,2,4],[1,2,4],[1,2,4]]
  pos_emb_dim: 512 

ddpm:
  max_step: 1000
  train_timestep: 1000
  noise_schedule: "torch.linspace(1e-6, 0.01, hparams.ddpm.max_step)"
  pos_emb_scale: 5000
  pos_emb_channels: 128 
  infer_step: 8 #TBD
  infer_schedule: "torch.tensor([1e-6,2e-6,1e-5,1e-4,1e-3,1e-2,1e-1,9e-1])" #TBD
  prediction_type: "epsilon" #epsilon; sample; v_prediction
  clip_sample: 'True'
  thresholding: 'False'
  clip_sample_range: 1.0

log:
  name: 'wavegrad2'
  checkpoint_dir: 'checkpoint'
  tensorboard_dir: 'tensorboard'
  test_result_dir: 'test_sample/result'
